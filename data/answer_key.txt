Answer 1
Factor Analysis is a statistical method used to identify the underlying relationships (called factors) between measured variables. It reduces the number of observed variables into fewer latent variables (factors) that explain most of the variance.

Steps of Factor Analysis:

Collect Data: Gather and prepare the data matrix with multiple variables.

Compute Correlation Matrix: Determine relationships among variables.

Extract Initial Factors: Use methods like Principal Component Analysis (PCA) to extract factors.

Determine Number of Factors: Based on eigenvalues or scree plot.

Factor Rotation: Apply techniques like Varimax to make the output more interpretable.

Interpret Factors: Analyze which variables load highly on which factors.

Validation: Use tools like factor scores and residuals to check adequacy.

Answer 2
An eigenvector of a square matrix is a non-zero vector that changes only in scale (not direction) when a linear transformation is applied.

Properties of Eigenvectors:

Linear Independence: Eigenvectors corresponding to distinct eigenvalues are linearly independent.

Direction Preservation: Eigenvectors maintain their direction after transformation.

Scalar Multiplication: Any scalar multiple of an eigenvector is still an eigenvector.

Associated with Eigenvalues: Each eigenvector is linked to a specific eigenvalue.

Useful in Dimensionality Reduction: Used in PCA for transforming data to a lower-dimensional space.


Answer 3
The principle of clustering is to group a set of data points into clusters such that:

Intra-cluster similarity is high (points within a cluster are similar).

Inter-cluster similarity is low (points from different clusters are dissimilar).

Clustering is an unsupervised learning technique used to discover patterns or structures in unlabeled data. It helps in applications like customer segmentation, document categorization, and image recognition.

Common algorithms include:

K-Means

Hierarchical Clustering

DBSCAN


Answer 4
High-dimensional datasets (many features/variables) introduce several challenges:

Curse of Dimensionality: As dimensions increase, data becomes sparse, making patterns harder to detect.

Increased Computation: More dimensions lead to higher processing time and memory usage.

Overfitting Risk: Models may fit noise instead of the actual pattern.

Visualization Difficulty: Data is hard to interpret visually in more than 3 dimensions.

Redundancy and Irrelevance: Not all features contribute meaningfully; many may be correlated or irrelevant.

Dimensionality reduction techniques like PCA or feature selection are used to address these issues.


Answer 5
Overfitting: The model learns the training data too well, including noise. It performs well on training data but poorly on unseen data.

Underfitting: The model is too simple and fails to capture the underlying trend, performing poorly on both training and test data.

Prevention Techniques:

For Overfitting:

Use cross-validation

Apply regularization (e.g., L1, L2)

Use simpler models

Reduce feature space

Add more training data

For Underfitting:

Use a more complex model

Increase training time

Add more relevant features

Balancing model complexity and training data is key to preventing both.